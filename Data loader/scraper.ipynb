{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium to call API via Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import json\n",
    "from time import sleep\n",
    "from typing import List, Dict\n",
    "from itertools import chain\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factories\n",
    "class WebDriverFactory:\n",
    "    '''\n",
    "    Factory class to create selenium web drivers\n",
    "\n",
    "    Attributes:\n",
    "        driver_path (str) : path of the Chrome driver executable\n",
    "        user_agent (str): fake user agent to add to the web driver\n",
    "        referrer (str) : fake referrer to add to the web driver\n",
    "        servce (selenium.webdriver.chrome.service)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.driver_path = \"C:\\\\Users\\\\joete\\\\Desktop\\\\chromedriver.exe\"\n",
    "        self.user_agent = 'User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "        self.referrer = 'Referrer=https://www.google.com/'\n",
    "        \n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument(self.user_agent)\n",
    "        self.options.add_argument(self.referrer )\n",
    "\n",
    "        self.service = Service(self.driver_path)\n",
    "\n",
    "    def create_driver(self) -> webdriver.Chrome:\n",
    "        '''\n",
    "        Args:\n",
    "            No arguments\n",
    "        Returns:\n",
    "            Selenium chrome driver with custom specifications\n",
    "        '''\n",
    "        return webdriver.Chrome(service = self.service, options = self.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domains\n",
    "class ReferenceData:\n",
    "    '''\n",
    "    Domain class to retrieve reference data for scraping\n",
    "\n",
    "    Attributes:\n",
    "        specials_url_path (str)\n",
    "        observation_list_path (str)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.specials_url_path = './specials_url.csv'\n",
    "        self.observation_list_path = './woolworths grocery monitor list.csv'\n",
    "    \n",
    "    def get_specials_urls(self) -> list:\n",
    "        \"\"\"\n",
    "        Get weekly specials urls for scraping\n",
    "\n",
    "        Returns:\n",
    "            list: list of specials urls\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.specials_url_path)\n",
    "        special_url_list = df[df['IN'] == 1]['loc'].to_list()\n",
    "        return special_url_list\n",
    "    \n",
    "    def get_observation_list(self) -> list:\n",
    "        \"\"\"\n",
    "        Get my grocery observation list\n",
    "\n",
    "        Returns:\n",
    "            list: list of SKUs that I am interested in\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.observation_list_path)\n",
    "        observation_list = df['sku'].tolist()\n",
    "        return observation_list\n",
    "\n",
    "class Scraper:\n",
    "    '''\n",
    "    Domain class to handle scraing logic\n",
    "\n",
    "    Attributes:\n",
    "        sku_api_endpoint (str): endpoint to retrieve sku related information\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.sku_api_endpoint = 'https://www.woolworths.com.au/api/v3/ui/schemaorg/product/{sku}'\n",
    "        self.sku_url_container = []\n",
    "\n",
    "    def get_sku_info(self, sku:int, driver: webdriver.Chrome) -> dict:\n",
    "        \"\"\"\n",
    "        Get SKU related information\n",
    "\n",
    "        Args:\n",
    "            sku (int): a particular sku\n",
    "            driver (webdriver.Chrome): webdriver responsible for calling the API endpoint\n",
    "\n",
    "        Returns:\n",
    "            dict: SKU information in JSON\n",
    "        \"\"\"\n",
    "        get_url = self.sku_api_endpoint.format(sku = sku)\n",
    "        driver.get(get_url)\n",
    "\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"/html/body/pre\")))\n",
    "\n",
    "        element = driver.find_element(By.XPATH, '/html/body/pre')\n",
    "        response = json.loads(element.text)\n",
    "\n",
    "        sleep(1)\n",
    "        return response\n",
    "    \n",
    "    def get_sku_url_from_page(self, driver:webdriver.Chrome , url:str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Getting the product page url of a particular sku from a grid of products\n",
    "\n",
    "        Args:\n",
    "            driver (webdriver.Chrome): selenium chrome driver to handle the scraping task\n",
    "            url (str): product grid page url\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: contains speicals category and SKU of products\n",
    "        \"\"\"\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        category = url.split('/')[-1].split('?')[0]\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'product-title-link.ng-star-inserted')))\n",
    "        \n",
    "        container_xpath = '//*[@id=\"search-content\"]/div/shared-grid'\n",
    "        product_class_name = 'product-title-link.ng-star-inserted'\n",
    "        container = driver.find_element(By.XPATH , container_xpath)\n",
    "        items = container.find_elements(By.CLASS_NAME, product_class_name)\n",
    "\n",
    "        for i in range(len(items)):\n",
    "            product_name = items[i].text\n",
    "            href = items[i].get_attribute('href')\n",
    "            if len(product_name) > 0:\n",
    "                ret.append(href.split('/')[-2])\n",
    "\n",
    "        df = pd.DataFrame({'category' : category, 'sku' : ret})\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_last_page_from_entry_point(self, driver:webdriver.Chrome, entry_url:str) -> int:\n",
    "        \"\"\"\n",
    "        Get last page number of a specials category\n",
    "\n",
    "        Args:\n",
    "            driver (webdriver.Chrome): selenium chrome driver to handle the scraping task\n",
    "            entry_url (str): the url of the first page of a special category\n",
    "\n",
    "        Returns:\n",
    "            int: the last page number of a special category\n",
    "        \"\"\"\n",
    "        last_page_class = 'paging-pageNumber'\n",
    "\n",
    "        driver.get(entry_url)\n",
    "\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'paging-pageNumber')))\n",
    "\n",
    "        last_page_number = int(driver.find_elements(By.CLASS_NAME, last_page_class)[-1].text.split()[-1])\n",
    "        sleep(1)\n",
    "\n",
    "        return last_page_number\n",
    "    \n",
    "    def get_all_page_urls_from_entry_point(self, driver: webdriver.Chrome, entry_url:str) -> list:\n",
    "        \"\"\"\n",
    "        Get all the page urls of a special category\n",
    "\n",
    "        Args:\n",
    "            driver (webdriver.Chrome): selenium chrome driver to handle the scraping task\n",
    "            entry_url (str): the url of the first page of a special category\n",
    "\n",
    "        Returns:\n",
    "            list: all pages urls of a special category\n",
    "        \"\"\"\n",
    "\n",
    "        last_page_number = self.get_last_page_from_entry_point(driver, entry_url)\n",
    "\n",
    "        ret = [entry_url+'?pageNumber='+str(i) for i in range(1,last_page_number+1)]\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controllers\n",
    "class ScraperController:\n",
    "    \"\"\"\n",
    "    Controller class to coordinate different scraping logic\n",
    "\n",
    "    Attributes:\n",
    "        max_workers: maximum number of workers to handle the scraping task\n",
    "        driver_factory: factory class object to create multiple selenium web drivers\n",
    "        drivers_container: list to contain all the selenium web drivers created by the driver_factory\n",
    "        scraper: domain class object to handle the scraping logic\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.max_workers = 8\n",
    "        self.driver_factory = WebDriverFactory()\n",
    "        # self.drivers_container = [self.driver_factory.create_driver() for i in range(self.max_workers)]\n",
    "        self.drivers_container = [self.driver_factory.create_driver() for i in range((self.max_workers+1)*2)]\n",
    "        self.scraper = Scraper()\n",
    "        self.specials_retry_urls= []\n",
    "\n",
    "    def get_sku_info_from_list(self, sku_list: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get SKU information from a list of SKU number, executed in a parallel fashion\n",
    "\n",
    "        Args:\n",
    "            sku_list (list): list of SKU ids\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: SKU information\n",
    "        \"\"\"\n",
    "\n",
    "        # initialise more twice as much driver than executors,\n",
    "        # alternately use the two batch of drivers for scrpaing to avoid stale element error\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            scraping_tasks = []\n",
    "\n",
    "            for i in range(len(sku_list)):\n",
    "                if (i//self.max_workers)%2 == 0:\n",
    "                    slot = i%self.max_workers+1\n",
    "                else:\n",
    "                    slot = (i%self.max_workers+1)+self.max_workers\n",
    "                scraping_tasks.append(executor.submit(self.scraper.get_sku_info, sku_list[i], self.drivers_container[slot]))\n",
    "\n",
    "            concurrent.futures.wait(scraping_tasks)\n",
    "\n",
    "        results = [task.result() for task in scraping_tasks if task.exception() is None]\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def get_all_page_url_from_specials_page(self) -> list:\n",
    "        \"\"\"\n",
    "        Get all page urls from specials category entry page\n",
    "\n",
    "        Returns:\n",
    "            list: all product grid page urls\n",
    "        \"\"\"\n",
    "\n",
    "        data = ReferenceData()\n",
    "        \n",
    "        special_urls = data.get_specials_urls()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            scraping_tasks = []\n",
    "\n",
    "            for i in range(len(special_urls)):\n",
    "                if (i//self.max_workers)%2 == 0:\n",
    "                    slot = i%self.max_workers+1\n",
    "                else:\n",
    "                    slot = (i%self.max_workers+1)+self.max_workers\n",
    "                scraping_tasks.append(executor.submit(self.scraper.get_all_page_urls_from_entry_point, self.drivers_container[slot], special_urls[i]))\n",
    "            \n",
    "            concurrent.futures.wait(scraping_tasks)\n",
    "\n",
    "        results = list(chain.from_iterable([task.result() for task in scraping_tasks if task.exception() is None]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_sepcial_sku(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all SKU informations that are on specials\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: all SKU informations that are on specials\n",
    "        \"\"\"\n",
    "\n",
    "        product_grid_page_urls = self.get_all_page_url_from_specials_page()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            scraping_tasks = []\n",
    "\n",
    "            for i in range(len(product_grid_page_urls)):\n",
    "                if (i//self.max_workers)%2 == 0:\n",
    "                    slot = i%self.max_workers+1\n",
    "                else:\n",
    "                    slot = (i%self.max_workers+1)+self.max_workers\n",
    "                scraping_tasks.append((executor.submit(self.scraper.get_sku_url_from_page, self.drivers_container[slot], product_grid_page_urls[i]), product_grid_page_urls[i])) # (task, product_grid_page_url[i])\n",
    "\n",
    "            concurrent.futures.wait([i[0] for i in scraping_tasks])\n",
    "\n",
    "        results = [task[0].result() for task in scraping_tasks if task[0].exception() is None]\n",
    "        failed_tasks_urls = [task[1] for task in scraping_tasks if task[0].exception() is not None]\n",
    "\n",
    "        # retry for failed tasks\n",
    "        for faied_url in failed_tasks_urls:\n",
    "            retried_sku_url_df = self.scraper.get_sku_url_from_page(self.drivers_container[0], faied_url)\n",
    "            results.append(retried_sku_url_df)\n",
    "\n",
    "        df = pd.concat(results)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_observation_sku_info(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get information of SKU that I am interested\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: SKU information\n",
    "        \"\"\"\n",
    "\n",
    "        data = ReferenceData()\n",
    "        \n",
    "        observation_url = data.get_observation_list()\n",
    "\n",
    "        ret = self.get_sku_info_from_list(observation_url)\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def close_drivers(self):\n",
    "        \"\"\"\n",
    "        Close all drivers initialised\n",
    "        \"\"\"\n",
    "        for driver in self.drivers_container:\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc  = ScraperController()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_sku_info = sc.get_observation_sku_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_skus = sc.get_all_sepcial_sku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials_sku_list = special_skus['sku'].to_list()\n",
    "special_sku_info = sc.get_sku_info_from_list(specials_sku_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.close_drivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
